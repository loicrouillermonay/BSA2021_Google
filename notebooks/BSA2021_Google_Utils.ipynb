{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_google = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Google')\n",
    "data_amazon = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Amazon')\n",
    "data_tesla = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Tesla')\n",
    "data_orange = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Orange')\n",
    "data_rolex = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Rolex')\n",
    "data_samsung = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Samsung')\n",
    "data_sbb = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'SBB')\n",
    "data_swisscom = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'Swisscom')\n",
    "data_ubs = pd.read_excel('../data/Data French difficulty.xls', sheet_name = 'UBS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_apple = pd.read_excel('../data/Data French difficulty.xls', sheet_name='Apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Google data as Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose data to put into train and validation datasets for each level of difficulty\n",
    "train_google_a1 = data_google[data_google['Difficulty'] == 'A1'].sample(frac = 0.9)\n",
    "validation_google_a1 = data_google[data_google['Difficulty'] == 'A1'].drop(train_google_a1.index)\n",
    "\n",
    "train_google_a2 = data_google[data_google['Difficulty'] == 'A2'].sample(frac = 0.9)\n",
    "validation_google_a2 = data_google[data_google['Difficulty'] == 'A2'].drop(train_google_a2.index)\n",
    "\n",
    "train_google_b1 = data_google[data_google['Difficulty'] == 'B1'].sample(frac = 0.9)\n",
    "validation_google_b1 = data_google[data_google['Difficulty'] == 'B1'].drop(train_google_b1.index)\n",
    "\n",
    "train_google_b2 = data_google[data_google['Difficulty'] == 'B2'].sample(frac = 0.9)\n",
    "validation_google_b2 = data_google[data_google['Difficulty'] == 'B2'].drop(train_google_b2.index)\n",
    "\n",
    "train_google_c1 = data_google[data_google['Difficulty'] == 'C1'].sample(frac = 0.9)\n",
    "validation_google_c1 = data_google[data_google['Difficulty'] == 'C1'].drop(train_google_c1.index)\n",
    "\n",
    "train_google_c2 = data_google[data_google['Difficulty'] == 'C2'].sample(frac = 0.9)\n",
    "validation_google_c2 = data_google[data_google['Difficulty'] == 'C2'].drop(train_google_c2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate chosen data into datasets\n",
    "full_train_google = pd.concat([train_google_a1, train_google_a2, train_google_b1, train_google_b2, train_google_c1, train_google_c2])\n",
    "full_validation_google = pd.concat([validation_google_a1, validation_google_a2, validation_google_b1, validation_google_b2, validation_google_c1, validation_google_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_validation_google' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-87f764211541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_validation_google\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDifficulty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'full_validation_google' is not defined"
     ]
    }
   ],
   "source": [
    "full_validation_google.Difficulty.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset into repository to avoid having to recompute them each time\n",
    "full_train_google.to_csv('./data/train-dataset.csv', index = False, header = False)\n",
    "full_validation_google.to_csv('./data/validation-dataset.csv', index = False, header = False)\n",
    "data_google.to_csv('./data/google-full-dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use other teams' datasets as Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open other groups' data and standardize the headers of some to correspond to our own format.\n",
    "data_amazon = data_amazon[['Phrases', 'Type (Aleks)']]\n",
    "data_amazon = data_amazon.rename(columns = {\"Phrases\": \"Text\", \"Type (Aleks)\": \"Difficulty\"})\n",
    "\n",
    "data_rolex = data_rolex[['Text', 'A0 Difficulty']]\n",
    "data_rolex = data_rolex.rename(columns = {\"A0 Difficulty\": \"Difficulty\"})\n",
    "\n",
    "data_swisscom = data_swisscom[['Texte ', 'Difficulty']]\n",
    "data_swisscom = data_swisscom.rename(columns = {\"Texte \": \"Text\"})\n",
    "\n",
    "data_tesla = data_tesla[['Text', 'Difficulty']]\n",
    "data_orange = data_orange[['Text', 'Difficulty']]\n",
    "data_rolex = data_rolex[['Text', 'Difficulty']]\n",
    "data_samsung = data_samsung[['Text', 'Difficulty']]\n",
    "data_sbb = data_sbb[['Text', 'Difficulty']]\n",
    "data_ubs = data_ubs[['Text', 'Difficulty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all those datasets into one\n",
    "dataset_test = pd.concat([data_amazon, data_rolex, data_swisscom, data_tesla, data_orange, data_rolex, data_samsung, data_sbb, data_ubs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "dataset_test = dataset_test.dropna()\n",
    "dataset_test = dataset_test.drop(dataset_test[dataset_test.Difficulty == 'BW'].index)\n",
    "dataset_test = dataset_test.reset_index(drop = True)\n",
    "dataset_test.Difficulty = dataset_test.Difficulty.replace(' B1', 'B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.Difficulty.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to the repository so that we don't have to recompute it each time\n",
    "dataset_test.to_csv('./data/test-dataset.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_apple.to_csv('../data/data_apple.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the data in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use other groups' data with ours and aggregate them into a giant dataset\n",
    "max_dataset = pd.concat([data_google, data_amazon, data_rolex, data_swisscom, data_tesla, data_orange, data_rolex, data_samsung, data_sbb, data_ubs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of the dataset\n",
    "max_dataset = max_dataset.dropna()\n",
    "max_dataset = max_dataset.drop(max_dataset[max_dataset.Difficulty == 'BW'].index)\n",
    "max_dataset = max_dataset.reset_index(drop = True)\n",
    "max_dataset.Difficulty = max_dataset.Difficulty.replace(' B1', 'B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dataset.Difficulty.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to the repository so that we don't have to recompute it each time\n",
    "max_dataset.to_csv('./data/max-dataset-no-header.csv', index = False, header = False)\n",
    "max_dataset.to_csv('./data/max-dataset.csv', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
