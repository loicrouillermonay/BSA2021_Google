{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BSA2021_Google - LingoRank with CamemBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QDd0_NbhwSb",
        "outputId": "a683c283-127e-4e7a-856b-36690ec8f931"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNS0ecn-cLBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b93be94-3d25-4e11-9754-81fc146598de"
      },
      "source": [
        "!pip install transformers==2.8.0\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 25.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 29.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 20.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 23.6MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 26.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 28.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 29.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 50.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/79/64c0815cbe8c6abd7fe5525ec37a2689d3cf10e387629ba4a6e44daff6d0/boto3-1.17.49-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 53.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Collecting botocore<1.21.0,>=1.20.49\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/59/6e28ce58206039ad2592992b75ee79a8f9dbc902a9704373ddacc4f96300/botocore-1.20.49-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 52.5MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.49->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=58054f457300042a51f87d20e8f67d80957cb7ca4fb6cc1ed9e49207c621a190\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.49 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, sentencepiece, jmespath, botocore, s3transfer, boto3, tokenizers, transformers\n",
            "Successfully installed boto3-1.17.49 botocore-1.20.49 jmespath-0.10.0 s3transfer-0.3.6 sacremoses-0.0.44 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKTsS8qmHKM0"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Functions : preprocess() (create dataloaders from raw data) \n",
        "# load_models() (load tokenizers and models) training() (loop of one training step) evaluate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLlAfPz-oUNB"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev)  \n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvPWbsrRa9nJ",
        "outputId": "fb24d89b-274b-4022-eaed-a2f780274543"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/max-dataset.csv\")\n",
        "dataset.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9174 entries, 0 to 9173\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Text        9174 non-null   object\n",
            " 1   Difficulty  9174 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 143.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shklKqs4cCyT"
      },
      "source": [
        "difficulties = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
        "for index, difficulty in zip(range(len(difficulties)), difficulties):\n",
        "    dataset['Difficulty'] = dataset['Difficulty'].replace([difficulty], index)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMDFTv5fGWG"
      },
      "source": [
        "# Split train-validation\n",
        "a1_X = dataset[dataset.Difficulty == 0]['Text']\n",
        "a2_X = dataset[dataset.Difficulty == 1]['Text']\n",
        "b1_X = dataset[dataset.Difficulty == 2]['Text']\n",
        "b2_X = dataset[dataset.Difficulty == 3]['Text']\n",
        "c1_X = dataset[dataset.Difficulty == 4]['Text']\n",
        "c2_X = dataset[dataset.Difficulty == 5]['Text']\n",
        "\n",
        "a1_y = dataset[dataset.Difficulty == 0]['Difficulty']\n",
        "a2_y = dataset[dataset.Difficulty == 1]['Difficulty']\n",
        "b1_y = dataset[dataset.Difficulty == 2]['Difficulty']\n",
        "b2_y = dataset[dataset.Difficulty == 3]['Difficulty']\n",
        "c1_y = dataset[dataset.Difficulty == 4]['Difficulty']\n",
        "c2_y = dataset[dataset.Difficulty == 5]['Difficulty']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk23lJWcfMZA"
      },
      "source": [
        "X_train_a1, X_test_a1, y_train_a1, y_test_a1 = train_test_split(a1_X, a1_y, test_size=0.1)\n",
        "X_train_a2, X_test_a2, y_train_a2, y_test_a2 = train_test_split(a2_X, a2_y, test_size=0.1)\n",
        "X_train_b1, X_test_b1, y_train_b1, y_test_b1 = train_test_split(b1_X, b1_y, test_size=0.1)\n",
        "X_train_b2, X_test_b2, y_train_b2, y_test_b2 = train_test_split(b2_X, b2_y, test_size=0.1)\n",
        "X_train_c1, X_test_c1, y_train_c1, y_test_c1 = train_test_split(c1_X, c1_y, test_size=0.1)\n",
        "X_train_c2, X_test_c2, y_train_c2, y_test_c2 = train_test_split(c2_X, c2_y, test_size=0.1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e7ZLss8fQnt"
      },
      "source": [
        "texts_train = X_train_a1.append([X_train_a2, X_train_b1, X_train_b2, X_train_c1, X_train_c2], ignore_index=True).values.tolist()\n",
        "texts_validation = X_test_a1.append([X_test_a2, X_test_b1, X_test_b2, X_test_c1, X_test_c2], ignore_index=True).values.tolist()\n",
        "labels_train = y_train_a1.append([y_train_a2, y_train_b1, y_train_b2, y_train_c1, y_train_c2], ignore_index=True).values.tolist()\n",
        "labels_validation = y_test_a1.append([y_test_a2, y_test_b1, y_test_b2, y_test_c1, y_test_c2], ignore_index=True).values.tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a5vW39cfH51"
      },
      "source": [
        "TOKENIZER = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfs9Ot9N4jdz"
      },
      "source": [
        "def preprocess(raw_texts, labels=None):\n",
        "    \"\"\"\n",
        "    Cette fonction prends de la donnée brute en argument et retourne un 'dataloader' pytorch\n",
        "\n",
        "    Args\n",
        "        raw_texts (array-like) : Une liste de texts sous forme de 'str'\n",
        "        \n",
        "        labels : Une liste 'labels' (0 = negatif, 1 = positif) de la meme taille que\n",
        "                     'raw_review'\n",
        "    \n",
        "    Returns\n",
        "        inputs_ids, attention_masks, labels(optionel) : Objet  de PyTorch qui contient \n",
        "                    les versions tokenisees et encodees des donnees brutes\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "\n",
        "    encoded_batch = TOKENIZER.batch_encode_plus(raw_texts,\n",
        "                                                add_special_tokens=True,\n",
        "                                                pad_to_max_length=True,\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_tensors = 'pt')\n",
        "    if labels:\n",
        "        labels = torch.tensor(labels)\n",
        "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], labels\n",
        "    return encoded_batch['input_ids'], encoded_batch['attention_mask']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Sm1dvCddc6"
      },
      "source": [
        "input_ids, attention_mask, labels_train = preprocess(texts_train, labels_train)\n",
        "# Combine the training inputs into a TensorDataset\n",
        "train_dataset = TensorDataset(\n",
        "    input_ids,\n",
        "    attention_mask,\n",
        "    labels_train)\n",
        "\n",
        "input_ids, attention_mask, labels_validation = preprocess(texts_validation, labels_validation)\n",
        "# Combine the validation inputs into a TensorDataset\n",
        "validation_dataset = TensorDataset(\n",
        "    input_ids,\n",
        "    attention_mask,\n",
        "    labels_validation)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaIdMxlirdFc"
      },
      "source": [
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size)\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            validation_dataset,\n",
        "            sampler = SequentialSampler(validation_dataset),\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoTafgwY_pyB",
        "outputId": "ebf880fb-0f25-40bc-b1b2-d167bb7d3df7"
      },
      "source": [
        "try:\n",
        "    state_dict = torch.load(\"/content/drive/MyDrive/Colab Notebooks/labelsfull.pt\")\n",
        "    print(\"Loading trained model...\")\n",
        "    model = CamembertForSequenceClassification.from_pretrained(\n",
        "    'camembert-base',\n",
        "    state_dict=state_dict,\n",
        "    num_labels = 6)\n",
        "    print(\"Trained model loaded!\")\n",
        "except Exception as e:\n",
        "    print(\"Enable to load trained model.\")\n",
        "    print(e)\n",
        "    model = CamembertForSequenceClassification.from_pretrained(\n",
        "        'camembert-base',\n",
        "        num_labels = 6)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading trained model...\n",
            "Trained model loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7vUl8O2gAUM"
      },
      "source": [
        "def predict(texts, model=model):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        input_ids, attention_mask = preprocess(texts)\n",
        "        retour = model(input_ids, attention_mask=attention_mask)\n",
        "        return torch.argmax(retour[0], dim=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjAyMcRW0bVW"
      },
      "source": [
        "def evaluate(texts, labels, metric='report'):\n",
        "    predictions = predict(texts)\n",
        "    if metric == 'report':\n",
        "        return metrics.classification_report(labels, predictions, zero_division=0)\n",
        "    elif metric == 'matrix':\n",
        "        return metrics.confusion_matrix(labels, predictions)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEvYTvehrgkY"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0DoJqlPwbZS"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # Learning Rate - Default is 5e-5\n",
        "                  eps = 1e-8 # Adam Epsilon  - Default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llBtKS_QbzXU"
      },
      "source": [
        "import gc \n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELf3ehrNxY3k",
        "outputId": "7f7574ff-d9e4-4624-ebb2-47ac8dac8a87"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev)  \n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "training_stats = []\n",
        "                                                                                \n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]\n",
        "# (Note that this is not the same as the number of training samples)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# This variable will evaluate the convergence on the training\n",
        "consecutive_epochs_with_no_improve = 0\n",
        "\n",
        "# Training\n",
        "for epoch in range(0, epochs):\n",
        "    \n",
        "    print(\"\")\n",
        "    print(f'########## Epoch {epoch} / {epochs} ##########')\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = time.time() - t0\n",
        "            \n",
        "            # Report progress\n",
        "            print(f'  Batch {step}  of  {len(train_dataloader)}    Elapsed: {format_time(elapsed)}.')\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the 'device' using the 'to' method\n",
        "        #\n",
        "        # 'batch' contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: skills \n",
        "        input_id = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Clear any previously calculated gradients before performing a backward pass\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch)\n",
        "        # the loss (because we provided skills) and the \"logits\"--the model\n",
        "        # outputs prior to activation\n",
        "        loss, logits = model(input_id, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=attention_mask, \n",
        "                             labels=labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. 'loss' is a Tensor containing a\n",
        "        # single value; the '.item()' function just returns the Python value \n",
        "        # from the tensor\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
        "\n",
        "    if epoch > 0:\n",
        "        if min([stat['Training Loss'] for stat in training_stats]) <= avg_train_loss:\n",
        "            # i.e. If there is not improvement\n",
        "            consecutive_epochs_with_no_improve += 1\n",
        "        else:\n",
        "            # If there is improvement\n",
        "            consecutive_epochs_with_no_improve = 0\n",
        "            print(\"Model saved!\")\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labelsfull.pt\")\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labelsfull.pth\")\n",
        "    \n",
        "    # Measure how long this epoch took\n",
        "    training_time = time.time() - t0\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "    \n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "    if consecutive_epochs_with_no_improve == 2:\n",
        "        print(\"Stop training : The loss has not changed since 2 epochs!\")\n",
        "        break\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Model saved!\")\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/metricsfull.json', 'w+') as outfile:\n",
        "    json.dump(training_stats, outfile)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labelsfull.pt\")\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labelsfull.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "########## Epoch 0 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:54.\n",
            "  Batch 200  of  516    Elapsed: 0:06:32.\n",
            "  Batch 250  of  516    Elapsed: 0:08:10.\n",
            "  Batch 300  of  516    Elapsed: 0:09:49.\n",
            "  Batch 350  of  516    Elapsed: 0:11:27.\n",
            "  Batch 400  of  516    Elapsed: 0:13:06.\n",
            "  Batch 450  of  516    Elapsed: 0:14:44.\n",
            "  Batch 500  of  516    Elapsed: 0:16:22.\n",
            "\n",
            "  Average training loss: 1.38\n",
            "  Training epoch took: 1013.6667642593384\n",
            "\n",
            "########## Epoch 1 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:38.\n",
            "  Batch 100  of  516    Elapsed: 0:03:16.\n",
            "  Batch 150  of  516    Elapsed: 0:04:54.\n",
            "  Batch 200  of  516    Elapsed: 0:06:31.\n",
            "  Batch 250  of  516    Elapsed: 0:08:08.\n",
            "  Batch 300  of  516    Elapsed: 0:09:46.\n",
            "  Batch 350  of  516    Elapsed: 0:11:23.\n",
            "  Batch 400  of  516    Elapsed: 0:13:00.\n",
            "  Batch 450  of  516    Elapsed: 0:14:37.\n",
            "  Batch 500  of  516    Elapsed: 0:16:15.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 1.03\n",
            "  Training epoch took: 1009.1443872451782\n",
            "\n",
            "########## Epoch 2 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:29.\n",
            "  Batch 250  of  516    Elapsed: 0:08:06.\n",
            "  Batch 300  of  516    Elapsed: 0:09:44.\n",
            "  Batch 350  of  516    Elapsed: 0:11:21.\n",
            "  Batch 400  of  516    Elapsed: 0:12:58.\n",
            "  Batch 450  of  516    Elapsed: 0:14:36.\n",
            "  Batch 500  of  516    Elapsed: 0:16:13.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 1007.447503566742\n",
            "\n",
            "########## Epoch 3 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:29.\n",
            "  Batch 250  of  516    Elapsed: 0:08:07.\n",
            "  Batch 300  of  516    Elapsed: 0:09:44.\n",
            "  Batch 350  of  516    Elapsed: 0:11:21.\n",
            "  Batch 400  of  516    Elapsed: 0:12:59.\n",
            "  Batch 450  of  516    Elapsed: 0:14:36.\n",
            "  Batch 500  of  516    Elapsed: 0:16:13.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epoch took: 1007.9027509689331\n",
            "\n",
            "########## Epoch 4 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:29.\n",
            "  Batch 250  of  516    Elapsed: 0:08:06.\n",
            "  Batch 300  of  516    Elapsed: 0:09:44.\n",
            "  Batch 350  of  516    Elapsed: 0:11:21.\n",
            "  Batch 400  of  516    Elapsed: 0:12:58.\n",
            "  Batch 450  of  516    Elapsed: 0:14:35.\n",
            "  Batch 500  of  516    Elapsed: 0:16:13.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 1007.4118452072144\n",
            "\n",
            "########## Epoch 5 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:29.\n",
            "  Batch 250  of  516    Elapsed: 0:08:07.\n",
            "  Batch 300  of  516    Elapsed: 0:09:44.\n",
            "  Batch 350  of  516    Elapsed: 0:11:21.\n",
            "  Batch 400  of  516    Elapsed: 0:12:59.\n",
            "  Batch 450  of  516    Elapsed: 0:14:36.\n",
            "  Batch 500  of  516    Elapsed: 0:16:14.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 1008.5365800857544\n",
            "\n",
            "########## Epoch 6 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:29.\n",
            "  Batch 250  of  516    Elapsed: 0:08:07.\n",
            "  Batch 300  of  516    Elapsed: 0:09:44.\n",
            "  Batch 350  of  516    Elapsed: 0:11:21.\n",
            "  Batch 400  of  516    Elapsed: 0:12:59.\n",
            "  Batch 450  of  516    Elapsed: 0:14:36.\n",
            "  Batch 500  of  516    Elapsed: 0:16:14.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epoch took: 1008.4199731349945\n",
            "\n",
            "########## Epoch 7 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:38.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:53.\n",
            "  Batch 200  of  516    Elapsed: 0:06:30.\n",
            "  Batch 250  of  516    Elapsed: 0:08:08.\n",
            "  Batch 300  of  516    Elapsed: 0:09:45.\n",
            "  Batch 350  of  516    Elapsed: 0:11:22.\n",
            "  Batch 400  of  516    Elapsed: 0:13:00.\n",
            "  Batch 450  of  516    Elapsed: 0:14:37.\n",
            "  Batch 500  of  516    Elapsed: 0:16:15.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 1009.4418785572052\n",
            "\n",
            "########## Epoch 8 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:30.\n",
            "  Batch 250  of  516    Elapsed: 0:08:07.\n",
            "  Batch 300  of  516    Elapsed: 0:09:45.\n",
            "  Batch 350  of  516    Elapsed: 0:11:22.\n",
            "  Batch 400  of  516    Elapsed: 0:12:59.\n",
            "  Batch 450  of  516    Elapsed: 0:14:37.\n",
            "  Batch 500  of  516    Elapsed: 0:16:14.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epoch took: 1008.9689471721649\n",
            "\n",
            "########## Epoch 9 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:15.\n",
            "  Batch 150  of  516    Elapsed: 0:04:52.\n",
            "  Batch 200  of  516    Elapsed: 0:06:29.\n",
            "  Batch 250  of  516    Elapsed: 0:08:07.\n",
            "  Batch 300  of  516    Elapsed: 0:09:44.\n",
            "  Batch 350  of  516    Elapsed: 0:11:22.\n",
            "  Batch 400  of  516    Elapsed: 0:13:00.\n",
            "  Batch 450  of  516    Elapsed: 0:14:39.\n",
            "  Batch 500  of  516    Elapsed: 0:16:17.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epoch took: 1012.7372748851776\n",
            "\n",
            "########## Epoch 10 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:38.\n",
            "  Batch 100  of  516    Elapsed: 0:03:17.\n",
            "  Batch 150  of  516    Elapsed: 0:04:55.\n",
            "  Batch 200  of  516    Elapsed: 0:06:34.\n",
            "  Batch 250  of  516    Elapsed: 0:08:13.\n",
            "  Batch 300  of  516    Elapsed: 0:09:52.\n",
            "  Batch 350  of  516    Elapsed: 0:11:31.\n",
            "  Batch 400  of  516    Elapsed: 0:13:09.\n",
            "  Batch 450  of  516    Elapsed: 0:14:48.\n",
            "  Batch 500  of  516    Elapsed: 0:16:26.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 1020.826292514801\n",
            "\n",
            "########## Epoch 11 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:37.\n",
            "  Batch 100  of  516    Elapsed: 0:03:14.\n",
            "  Batch 150  of  516    Elapsed: 0:04:51.\n",
            "  Batch 200  of  516    Elapsed: 0:06:27.\n",
            "  Batch 250  of  516    Elapsed: 0:08:04.\n",
            "  Batch 300  of  516    Elapsed: 0:09:41.\n",
            "  Batch 350  of  516    Elapsed: 0:11:17.\n",
            "  Batch 400  of  516    Elapsed: 0:12:54.\n",
            "  Batch 450  of  516    Elapsed: 0:14:31.\n",
            "  Batch 500  of  516    Elapsed: 0:16:07.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epoch took: 1002.3978321552277\n",
            "\n",
            "########## Epoch 12 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  516    Elapsed: 0:01:36.\n",
            "  Batch 100  of  516    Elapsed: 0:03:13.\n",
            "  Batch 150  of  516    Elapsed: 0:04:49.\n",
            "  Batch 200  of  516    Elapsed: 0:06:26.\n",
            "  Batch 250  of  516    Elapsed: 0:08:02.\n",
            "  Batch 300  of  516    Elapsed: 0:09:39.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcXrU66sICP"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "# Evaluation with the confusion matrix\n",
        "import seaborn\n",
        "confusion_matrix = evaluate(texts_validation, labels_validation, 'matrix')\n",
        "report = evaluate(texts_validation, labels_validation, 'report')\n",
        "print(report)\n",
        "seaborn.heatmap(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j-DNVPX7VnO",
        "outputId": "159279de-edaf-417c-a786-db7ca88e4091"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "920"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0e_ENz8_Eq"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "predictions = []\n",
        "for sentence in texts_validation:\n",
        "    predictions.append(predict([sentence]))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Yi0Wb--tAG",
        "outputId": "51a937dd-6c01-45fc-fe57-99bf826b45d1"
      },
      "source": [
        "temp_predictions"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5djlXFzdOv7Z"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "predictions = predict(texts_validation[600:])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HId_35BKPUlw",
        "outputId": "594f2a5f-0b00-40da-9f1e-43a40bf12924"
      },
      "source": [
        "print(metrics.classification_report(predictions, labels_validation))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96       150\n",
            "           1       0.93      0.94      0.93       175\n",
            "           2       0.93      0.93      0.93       153\n",
            "           3       0.95      0.95      0.95       157\n",
            "           4       0.93      0.97      0.95       154\n",
            "           5       0.97      0.97      0.97       131\n",
            "\n",
            "    accuracy                           0.95       920\n",
            "   macro avg       0.95      0.95      0.95       920\n",
            "weighted avg       0.95      0.95      0.95       920\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yF0X_utPJhT",
        "outputId": "25a867e8-adc6-4347-834f-e21d447e49ca"
      },
      "source": [
        "metrics.confusion_matrix(predictions, labels_validation)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[140,   8,   2,   0,   0,   0],\n",
              "       [  2, 165,   7,   1,   0,   0],\n",
              "       [  0,   3, 142,   6,   2,   0],\n",
              "       [  0,   2,   1, 149,   5,   0],\n",
              "       [  0,   0,   0,   1, 149,   4],\n",
              "       [  0,   0,   0,   0,   4, 127]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCRFLtf4Kkg2",
        "outputId": "44a46d32-b837-45d3-c265-3200baf0317b"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "predict([\"Dans un premier temps, nous nous demanderons si le travail n’est qu’une activité imposée par l’extérieur contre la volonté de l’Homme, puis dans un deuxième temps nous nous interrogerons sur le fait que le travail est une activité que l’être humain s’impose librement à lui-même.\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sdo6ELiKTbZ",
        "outputId": "2f27a4dd-582d-4d2f-c660-d88138c10738"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "predict(\"Dans un premier temps, nous nous demanderons si le travail n’est qu’une activité imposée par l’extérieur contre la volonté de l’Homme, puis dans un deuxième temps nous nous interrogerons sur le fait que le travail est une activité que l’être humain s’impose librement à lui-même.\".split(' '))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NAjNyL89Ieh"
      },
      "source": [
        "# Full training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lfPXVChjhGE"
      },
      "source": [
        "texts = dataset.Text.values.tolist()\n",
        "labels = dataset.Difficulty.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAfsDfuO-D30"
      },
      "source": [
        "input_ids, attention_mask, labels = preprocess(texts, labels)\n",
        "# Combine the training inputs into a TensorDataset\n",
        "full_train_dataset = TensorDataset(\n",
        "    input_ids,\n",
        "    attention_mask,\n",
        "    labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWXp8_ho-RLM"
      },
      "source": [
        "# size of 16 or 32.\n",
        "batch_size = 4\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "            full_train_dataset,\n",
        "            sampler = RandomSampler(full_train_dataset),\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1IWiNmj9KP4",
        "outputId": "9e99277a-2478-4ddf-cfba-2c89fa8cb64e"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev)  \n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "training_stats = []\n",
        "                                                                                \n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]\n",
        "# (Note that this is not the same as the number of training samples)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# This variable will evaluate the convergence on the training\n",
        "consecutive_epochs_with_no_improve = 0\n",
        "\n",
        "# Training\n",
        "for epoch in range(0, epochs):\n",
        "    \n",
        "    print(\"\")\n",
        "    print(f'########## Epoch {epoch} / {epochs} ##########')\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = time.time() - t0\n",
        "            \n",
        "            # Report progress\n",
        "            print(f'  Batch {step}  of  {len(train_dataloader)}    Elapsed: {format_time(elapsed)}.')\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the 'device' using the 'to' method\n",
        "        #\n",
        "        # 'batch' contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: skills \n",
        "        input_id = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Clear any previously calculated gradients before performing a backward pass\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch)\n",
        "        # the loss (because we provided skills) and the \"logits\"--the model\n",
        "        # outputs prior to activation\n",
        "        loss, logits = model(input_id, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=attention_mask, \n",
        "                             labels=labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. 'loss' is a Tensor containing a\n",
        "        # single value; the '.item()' function just returns the Python value \n",
        "        # from the tensor\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
        "\n",
        "    if epoch > 0:\n",
        "        if min([stat['Training Loss'] for stat in training_stats]) <= avg_train_loss:\n",
        "            # i.e. If there is not improvement\n",
        "            consecutive_epochs_with_no_improve += 1\n",
        "        else:\n",
        "            # If there is improvement\n",
        "            consecutive_epochs_with_no_improve = 0\n",
        "            print(\"Model saved!\")\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labels.pt\")\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labels.pth\")\n",
        "    \n",
        "    # Measure how long this epoch took\n",
        "    training_time = time.time() - t0\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "    \n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "    if consecutive_epochs_with_no_improve == 2:\n",
        "        print(\"Stop training : The loss has not changed since 2 epochs!\")\n",
        "        break\n",
        "\n",
        "print(\"Model saved!\")\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/metrics.json', 'w+') as outfile:\n",
        "    json.dump(training_stats, outfile)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/labels.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "########## Epoch 0 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:25.\n",
            "  Batch 250  of  255    Elapsed: 0:00:31.\n",
            "\n",
            "  Average training loss: 1.57\n",
            "  Training epoch took: 31.357661962509155\n",
            "\n",
            "########## Epoch 1 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:31.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 1.14\n",
            "  Training epoch took: 35.01443815231323\n",
            "\n",
            "########## Epoch 2 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:31.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epoch took: 34.79797649383545\n",
            "\n",
            "########## Epoch 3 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:19.\n",
            "  Batch 200  of  255    Elapsed: 0:00:25.\n",
            "  Batch 250  of  255    Elapsed: 0:00:31.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 34.87287163734436\n",
            "\n",
            "########## Epoch 4 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epoch took: 34.514421463012695\n",
            "\n",
            "########## Epoch 5 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epoch took: 34.3389892578125\n",
            "\n",
            "########## Epoch 6 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epoch took: 34.125643253326416\n",
            "\n",
            "########## Epoch 7 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epoch took: 34.12381672859192\n",
            "\n",
            "########## Epoch 8 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epoch took: 33.93142867088318\n",
            "\n",
            "########## Epoch 9 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epoch took: 34.85497736930847\n",
            "\n",
            "########## Epoch 10 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "Model saved!\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 34.03747868537903\n",
            "\n",
            "########## Epoch 11 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:30.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 30.31814932823181\n",
            "\n",
            "########## Epoch 12 / 20 ##########\n",
            "Training...\n",
            "  Batch 50  of  255    Elapsed: 0:00:06.\n",
            "  Batch 100  of  255    Elapsed: 0:00:12.\n",
            "  Batch 150  of  255    Elapsed: 0:00:18.\n",
            "  Batch 200  of  255    Elapsed: 0:00:24.\n",
            "  Batch 250  of  255    Elapsed: 0:00:29.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 30.07325005531311\n",
            "Stop training : The loss has not changed since 2 epochs!\n",
            "Model saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBKi25SWQjIJ"
      },
      "source": [
        "# Full test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx7Qwm8SXpy3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_pNBPfJcyP-"
      },
      "source": [
        "test_dataset = pd.read_csv('/content/test-dataset-header.csv')\n",
        "\n",
        "difficulties = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
        "for index, difficulty in zip(range(len(difficulties)), difficulties):\n",
        "    test_dataset['Difficulty'] = test_dataset['Difficulty'].replace([difficulty], index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTMiSEnm9oDq"
      },
      "source": [
        "texts = test_dataset.Text.values.tolist()\n",
        "labels = test_dataset.Difficulty.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTDpX54idHwN",
        "outputId": "222553e5-28d5-45f6-8e0e-b5539c08c45c"
      },
      "source": [
        "try:\n",
        "    state_dict = torch.load(\"/content/drive/MyDrive/Colab Notebooks/labels.pt\")\n",
        "    print(\"Loading trained model...\")\n",
        "    model = CamembertForSequenceClassification.from_pretrained(\n",
        "    'camembert-base',\n",
        "    state_dict=state_dict,\n",
        "    num_labels = 6)\n",
        "    print(\"Trained model loaded!\")\n",
        "except Exception as e:\n",
        "    print(\"Enable to load trained model.\")\n",
        "    print(e)\n",
        "    model = CamembertForSequenceClassification.from_pretrained(\n",
        "        'camembert-base',\n",
        "        num_labels = 6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading trained model...\n",
            "Trained model loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-RhOslojMLR",
        "outputId": "3edc9a05-3f50-440d-d1a3-a75f91217168"
      },
      "source": [
        "int(len(texts)/12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdmQYNcid3um"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "half = int(len(texts)/12)\n",
        "predictions = predict(texts[6000:7000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlQvNWXQeVgh",
        "outputId": "8e707bc8-4a40-44cb-f6bf-6cc0ef2925f8"
      },
      "source": [
        "print(metrics.classification_report(predictions, labels[6000:7000]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.13      0.43      0.20        35\n",
            "           1       0.62      0.30      0.41       410\n",
            "           2       0.15      0.23      0.18       151\n",
            "           3       0.56      0.52      0.54       246\n",
            "           4       0.32      0.50      0.39       131\n",
            "           5       0.00      0.00      0.00        27\n",
            "\n",
            "    accuracy                           0.37      1000\n",
            "   macro avg       0.30      0.33      0.29      1000\n",
            "weighted avg       0.46      0.37      0.39      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNi-bE_hk970",
        "outputId": "f14cb7e1-2964-499e-f986-f1e12c9e22f9"
      },
      "source": [
        "print(metrics.confusion_matrix(predictions, labels[6000:7000]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 15  12   7   1   0   0]\n",
            " [ 75 124 144  50  13   4]\n",
            " [ 17  41  34  26  29   4]\n",
            " [  3  16  18 128  71  10]\n",
            " [  4   6  17  22  65  17]\n",
            " [  0   1   0   1  25   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr11xGq9lZZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uR1eugZCoc5"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R6lTgJECoIK"
      },
      "source": [
        "data_apple = pd.read_csv('/content/data_apple.csv')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7Pw7MIm6CwcE",
        "outputId": "0da22bd5-f058-4b6a-d6f0-97aeac99292d"
      },
      "source": [
        "data_apple"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Difficulty</th>\n",
              "      <th>Difficulty Annotator 1</th>\n",
              "      <th>Difficulty Annotator 2</th>\n",
              "      <th>Difficulty Annotator 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Franck est français.</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Il est employé</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Il habite en Italie avec sa femme et ses enfants.</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Il travaille à Rome</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Il parle italien et anglais</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>Si la connaissance est convoquée une troisième...</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>« Parce que les révisions aident à faire retro...</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>Un enfant peut avoir parfois l’impression d’av...</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>Un simple indice donné durant les vacances va ...</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>Revue une fois, elle sera cette fois plus faci...</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C1</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1119 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text  ... Difficulty Annotator 3\n",
              "0                                  Franck est français.  ...                     A1\n",
              "1                                        Il est employé  ...                     A1\n",
              "2     Il habite en Italie avec sa femme et ses enfants.  ...                     A1\n",
              "3                                   Il travaille à Rome  ...                     A1\n",
              "4                           Il parle italien et anglais  ...                     A1\n",
              "...                                                 ...  ...                    ...\n",
              "1114  Si la connaissance est convoquée une troisième...  ...                     C2\n",
              "1115  « Parce que les révisions aident à faire retro...  ...                     C2\n",
              "1116  Un enfant peut avoir parfois l’impression d’av...  ...                     C2\n",
              "1117  Un simple indice donné durant les vacances va ...  ...                     C2\n",
              "1118  Revue une fois, elle sera cette fois plus faci...  ...                     C1\n",
              "\n",
              "[1119 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_POkCnscCxYi"
      },
      "source": [
        "difficulties = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
        "for index, difficulty in zip(range(len(difficulties)), difficulties):\n",
        "    data_apple['Difficulty'] = data_apple['Difficulty'].replace([difficulty], index)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "6ioOKirPDAzf",
        "outputId": "2e85dbe9-3fd4-4dcd-b131-cad583e13c98"
      },
      "source": [
        "data_apple"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Difficulty</th>\n",
              "      <th>Difficulty Annotator 1</th>\n",
              "      <th>Difficulty Annotator 2</th>\n",
              "      <th>Difficulty Annotator 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Franck est français.</td>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Il est employé</td>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Il habite en Italie avec sa femme et ses enfants.</td>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Il travaille à Rome</td>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Il parle italien et anglais</td>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>Si la connaissance est convoquée une troisième...</td>\n",
              "      <td>5</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>« Parce que les révisions aident à faire retro...</td>\n",
              "      <td>5</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>Un enfant peut avoir parfois l’impression d’av...</td>\n",
              "      <td>5</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>Un simple indice donné durant les vacances va ...</td>\n",
              "      <td>5</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>Revue une fois, elle sera cette fois plus faci...</td>\n",
              "      <td>5</td>\n",
              "      <td>C2</td>\n",
              "      <td>C1</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1119 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text  ...  Difficulty Annotator 3\n",
              "0                                  Franck est français.  ...                      A1\n",
              "1                                        Il est employé  ...                      A1\n",
              "2     Il habite en Italie avec sa femme et ses enfants.  ...                      A1\n",
              "3                                   Il travaille à Rome  ...                      A1\n",
              "4                           Il parle italien et anglais  ...                      A1\n",
              "...                                                 ...  ...                     ...\n",
              "1114  Si la connaissance est convoquée une troisième...  ...                      C2\n",
              "1115  « Parce que les révisions aident à faire retro...  ...                      C2\n",
              "1116  Un enfant peut avoir parfois l’impression d’av...  ...                      C2\n",
              "1117  Un simple indice donné durant les vacances va ...  ...                      C2\n",
              "1118  Revue une fois, elle sera cette fois plus faci...  ...                      C1\n",
              "\n",
              "[1119 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptPzvhYzDBX7"
      },
      "source": [
        "device = torch.device('cpu') \n",
        "model.to(device)\n",
        "\n",
        "predictions = []\n",
        "for sentence in data_apple['Text']:\n",
        "    predictions.append(predict([sentence]))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h72gdWb7DOS2",
        "outputId": "e4adf4b8-6794-4562-b70e-ab89de33ff9a"
      },
      "source": [
        "print(metrics.classification_report(data_apple['Difficulty'], predictions))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76       192\n",
            "           1       0.53      0.90      0.67       205\n",
            "           2       0.53      0.90      0.67       174\n",
            "           3       0.41      0.31      0.35       167\n",
            "           4       0.56      0.33      0.41       199\n",
            "           5       1.00      0.30      0.46       182\n",
            "\n",
            "    accuracy                           0.58      1119\n",
            "   macro avg       0.64      0.58      0.55      1119\n",
            "weighted avg       0.64      0.58      0.56      1119\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha_sH4nYDTxo",
        "outputId": "8d98bfdb-2363-4fc2-8be8-7caac5e9fc99"
      },
      "source": [
        "print(metrics.confusion_matrix(data_apple['Difficulty'], predictions))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[144  41   7   0   0   0]\n",
            " [ 12 184   5   3   1   0]\n",
            " [  5  13 156   0   0   0]\n",
            " [ 11  45  53  51   7   0]\n",
            " [ 13  58  39  24  65   0]\n",
            " [  1   5  33  46  43  54]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY9a7RfEECIf",
        "outputId": "0fd9ae3a-5cc2-45d6-9d63-0abbe2c3f29e"
      },
      "source": [
        "for x, y, z in zip(predictions, data_apple.Difficulty, data_apple.Text):\n",
        "    # print(int(x), y, z)\n",
        "    if int(x) == 1:\n",
        "        if y == 5:\n",
        "            print(z, x, y)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Les valorisations boursières des sociétés Internet comme AMAZON tensor([1]) 5\n",
            "\"Certes, un rêve de beignet, c’est un rêve, pas un beignet tensor([1]) 5\n",
            "Mais à qui la faute ? À l'histoire, d'abord tensor([1]) 5\n",
            "Je travaille tous les jours en anglais, et je suis souvent publié par des journaux américains tensor([1]) 5\n",
            "Voici un extrait de sa conférence à Lannion, le 28 mai 1988 tensor([1]) 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcU-NyrhEmrO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTtBqzyUFvty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-KJoMYYFvoz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQW_SOZeFvjJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxpZ533OFvd6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "621G9sboFvai"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir86IQl9FvVd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeKq5sZTFvQO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnvebP20FvJb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDCD1sOdFvCr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIar6EhZFu9q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaKk3BnBFu4N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNuUhkroFuym"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqX5kRiiFutN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq6FiAMvFug3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ahe8HPYFudM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}