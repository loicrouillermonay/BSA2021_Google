{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QDd0_NbhwSb",
    "outputId": "96a71335-e0c7-4716-b5fd-a6c28b9b9f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNS0ecn-cLBJ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.8.0\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKTsS8qmHKM0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "punctuation = string.punctuation\n",
    "# Credits to Olivier. (2021, January 5). Analyse de sentiments avec CamemBERT. Le Data Scientist. https://ledatascientist.com/analyse-de-sentiments-avec-camembert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvPWbsrRa9nJ",
    "outputId": "e7a52dc9-fe9a-438b-938a-f28a06bd7c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9174 entries, 0 to 9173\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Text        9174 non-null   object\n",
      " 1   Difficulty  9174 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 143.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/content/max-dataset.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shklKqs4cCyT"
   },
   "outputs": [],
   "source": [
    "# Transpore A1-C2 scale into 0 to 5 \n",
    "difficulties = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "for index, difficulty in zip(range(len(difficulties)), difficulties):\n",
    "    dataset['Difficulty'] = dataset['Difficulty'].replace([difficulty], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe53y0spnwdD"
   },
   "outputs": [],
   "source": [
    "# text preprocessing - remove punctuation and lowercase\n",
    "punctuation = punctuation.replace('-', '')\n",
    "punctuation = punctuation.replace('~', '')\n",
    "\n",
    "def preprocess_text(text: string):\n",
    "    sentence = ''.join([ word for word in text if word not in punctuation ])\n",
    "    return (sentence).lower()\n",
    "\n",
    "dataset.Text = dataset.Text.apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnMDFTv5fGWG"
   },
   "outputs": [],
   "source": [
    "# Filter by target variable in order to create a balanced train and test set.\n",
    "a1_X = dataset[dataset.Difficulty == 0]['Text']\n",
    "a2_X = dataset[dataset.Difficulty == 1]['Text']\n",
    "b1_X = dataset[dataset.Difficulty == 2]['Text']\n",
    "b2_X = dataset[dataset.Difficulty == 3]['Text']\n",
    "c1_X = dataset[dataset.Difficulty == 4]['Text']\n",
    "c2_X = dataset[dataset.Difficulty == 5]['Text']\n",
    "\n",
    "a1_y = dataset[dataset.Difficulty == 0]['Difficulty']\n",
    "a2_y = dataset[dataset.Difficulty == 1]['Difficulty']\n",
    "b1_y = dataset[dataset.Difficulty == 2]['Difficulty']\n",
    "b2_y = dataset[dataset.Difficulty == 3]['Difficulty']\n",
    "c1_y = dataset[dataset.Difficulty == 4]['Difficulty']\n",
    "c2_y = dataset[dataset.Difficulty == 5]['Difficulty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk23lJWcfMZA"
   },
   "outputs": [],
   "source": [
    "# Train_test_split for each target variable\n",
    "X_train_a1, X_test_a1, y_train_a1, y_test_a1 = train_test_split(a1_X, a1_y, test_size=0.1, random_state=707)\n",
    "X_train_a2, X_test_a2, y_train_a2, y_test_a2 = train_test_split(a2_X, a2_y, test_size=0.1, random_state=707)\n",
    "X_train_b1, X_test_b1, y_train_b1, y_test_b1 = train_test_split(b1_X, b1_y, test_size=0.1, random_state=707)\n",
    "X_train_b2, X_test_b2, y_train_b2, y_test_b2 = train_test_split(b2_X, b2_y, test_size=0.1, random_state=707)\n",
    "X_train_c1, X_test_c1, y_train_c1, y_test_c1 = train_test_split(c1_X, c1_y, test_size=0.1, random_state=707)\n",
    "X_train_c2, X_test_c2, y_train_c2, y_test_c2 = train_test_split(c2_X, c2_y, test_size=0.1, random_state=707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e7ZLss8fQnt"
   },
   "outputs": [],
   "source": [
    "# Group X, y\n",
    "texts_train = X_train_a1.append([X_train_a2, X_train_b1, X_train_b2, X_train_c1, X_train_c2], ignore_index=True).values.tolist()\n",
    "texts_test = X_test_a1.append([X_test_a2, X_test_b1, X_test_b2, X_test_c1, X_test_c2], ignore_index=True).values.tolist()\n",
    "labels_train = y_train_a1.append([y_train_a2, y_train_b1, y_train_b2, y_train_c1, y_train_c2], ignore_index=True).values.tolist()\n",
    "labels_test = y_test_a1.append([y_test_a2, y_test_b1, y_test_b2, y_test_c1, y_test_c2], ignore_index=True).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6330973987784c2ab0770e421567aee8",
      "02d2152201684bda9a3b1bde794166f3",
      "e6e45b357c334cae955efd3eace3bb4d",
      "0c2fd3d8311e43b28755b04f8e7835b9",
      "f3e5447e64a54787a58ac4e0d214ad1a",
      "4a0f60a85b6a487e89289827af5bc383",
      "41288f2b2c10450194ee7e23f8fb28f3",
      "c1e2470a159b4afca931d1966010628e"
     ]
    },
    "id": "6a5vW39cfH51",
    "outputId": "dca5eb0e-0455-4482-bf7c-bff8a4515d90"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6330973987784c2ab0770e421567aee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=810912.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sfs9Ot9N4jdz"
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_texts, labels=None):\n",
    "    \"\"\"\n",
    "    Takes raw data as argument and returns a pytorch dataloader.\n",
    "\n",
    "    Args\n",
    "        raw_texts (array-like) : A list of texts in the form of 'str'\n",
    "        \n",
    "        labels : a labels list from 0 to 5\n",
    "    \n",
    "    Returns\n",
    "        inputs_ids, attention_masks, labels(optionel) : PyTorch object that contains tokenized and encoded versions of raw data\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_texts,\n",
    "                                                add_special_tokens=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if labels:\n",
    "        labels = torch.tensor(labels)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], labels\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_Sm1dvCddc6"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_mask, labels_train = preprocess(texts_train, labels_train)\n",
    "# Combine the training inputs into a TensorDataset\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    labels_train)\n",
    "\n",
    "input_ids, attention_mask, labels_test = preprocess(texts_test, labels_test)\n",
    "# Combine the validation inputs into a TensorDataset\n",
    "validation_dataset = TensorDataset(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaIdMxlirdFc"
   },
   "outputs": [],
   "source": [
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset,\n",
    "            sampler = SequentialSampler(validation_dataset),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoTafgwY_pyB",
    "outputId": "f393f316-2990-4768-e7b5-2d35d6a6b26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n",
      "Trained model loaded!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    state_dict = torch.load(\"/content/drive/MyDrive/Colab Notebooks/lingorank-v3.pt\")\n",
    "    print(\"Loading trained model...\")\n",
    "    model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    state_dict=state_dict,\n",
    "    num_labels = 6)\n",
    "    print(\"Trained model loaded!\")\n",
    "except Exception as e:\n",
    "    print(\"Enable to load trained model.\")\n",
    "    print(e)\n",
    "    model = CamembertForSequenceClassification.from_pretrained(\n",
    "        'camembert-base',\n",
    "        num_labels = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7vUl8O2gAUM"
   },
   "outputs": [],
   "source": [
    "def predict(texts, model=model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(texts)\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "        return torch.argmax(retour[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjAyMcRW0bVW"
   },
   "outputs": [],
   "source": [
    "def evaluate(texts, labels, metric='report'):\n",
    "    predictions = predict(texts)\n",
    "    if metric == 'report':\n",
    "        return metrics.classification_report(labels, predictions, zero_division=0)\n",
    "    elif metric == 'matrix':\n",
    "        return metrics.confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEvYTvehrgkY"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0DoJqlPwbZS"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate - Default is 5e-5\n",
    "                  eps = 1e-8 # Adam Epsilon  - Default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llBtKS_QbzXU"
   },
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELf3ehrNxY3k",
    "outputId": "d58035c0-d4bb-42e1-fada-c2796066c889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Epoch 0 / 10 ##########\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 50  of  516    Elapsed: 0:00:44.\n",
      "  Batch 100  of  516    Elapsed: 0:01:30.\n",
      "  Batch 150  of  516    Elapsed: 0:02:17.\n",
      "  Batch 200  of  516    Elapsed: 0:03:05.\n",
      "  Batch 250  of  516    Elapsed: 0:03:54.\n",
      "  Batch 300  of  516    Elapsed: 0:04:42.\n",
      "  Batch 350  of  516    Elapsed: 0:05:31.\n",
      "  Batch 400  of  516    Elapsed: 0:06:20.\n",
      "  Batch 450  of  516    Elapsed: 0:07:10.\n",
      "  Batch 500  of  516    Elapsed: 0:07:59.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 494.74662923812866\n",
      "\n",
      "########## Epoch 1 / 10 ##########\n",
      "Training...\n",
      "  Batch 50  of  516    Elapsed: 0:00:49.\n",
      "  Batch 100  of  516    Elapsed: 0:01:39.\n",
      "  Batch 150  of  516    Elapsed: 0:02:28.\n",
      "  Batch 200  of  516    Elapsed: 0:03:17.\n",
      "  Batch 250  of  516    Elapsed: 0:04:07.\n",
      "  Batch 300  of  516    Elapsed: 0:04:56.\n",
      "  Batch 350  of  516    Elapsed: 0:05:45.\n",
      "  Batch 400  of  516    Elapsed: 0:06:35.\n",
      "  Batch 450  of  516    Elapsed: 0:07:24.\n",
      "  Batch 500  of  516    Elapsed: 0:08:13.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 509.149747133255\n",
      "\n",
      "########## Epoch 2 / 10 ##########\n",
      "Training...\n",
      "  Batch 50  of  516    Elapsed: 0:00:49.\n",
      "  Batch 100  of  516    Elapsed: 0:01:39.\n",
      "  Batch 150  of  516    Elapsed: 0:02:28.\n",
      "  Batch 200  of  516    Elapsed: 0:03:17.\n",
      "  Batch 250  of  516    Elapsed: 0:04:07.\n",
      "  Batch 300  of  516    Elapsed: 0:04:56.\n",
      "  Batch 350  of  516    Elapsed: 0:05:46.\n",
      "  Batch 400  of  516    Elapsed: 0:06:35.\n",
      "  Batch 450  of  516    Elapsed: 0:07:25.\n",
      "  Batch 500  of  516    Elapsed: 0:08:14.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 509.6893136501312\n",
      "Stop training : The loss has not changed since 2 epochs!\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)  \n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "training_stats = []\n",
    "                                                                                \n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]\n",
    "# (Note that this is not the same as the number of training samples)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# This variable will evaluate the convergence on the training\n",
    "consecutive_epochs_with_no_improve = 0\n",
    "\n",
    "# Training\n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch} / {epochs} ##########')\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = time.time() - t0\n",
    "            \n",
    "            # Report progress\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}    Elapsed: {format_time(elapsed)}.')\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the 'device' using the 'to' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: skills \n",
    "        input_id = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear any previously calculated gradients before performing a backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch)\n",
    "        # the loss (because we provided skills) and the \"logits\"--the model\n",
    "        # outputs prior to activation\n",
    "        loss, logits = model(input_id, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. 'loss' is a Tensor containing a\n",
    "        # single value; the '.item()' function just returns the Python value \n",
    "        # from the tensor\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    "\n",
    "    if epoch > 0:\n",
    "        if min([stat['Training Loss'] for stat in training_stats]) <= avg_train_loss:\n",
    "            # i.e. If there is not improvement\n",
    "            consecutive_epochs_with_no_improve += 1\n",
    "        else:\n",
    "            # If there is improvement\n",
    "            consecutive_epochs_with_no_improve = 0\n",
    "            print(\"Model saved!\")\n",
    "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/lingorank-v3.pt\")\n",
    "    \n",
    "    # Measure how long this epoch took\n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "    if consecutive_epochs_with_no_improve == 2:\n",
    "        print(\"Stop training : The loss has not changed since 2 epochs!\")\n",
    "        break\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Model saved!\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/metrics-v3.json', 'w+') as outfile:\n",
    "    json.dump(training_stats, outfile)\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/lingorank-v3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb0e_ENz8_Eq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu') \n",
    "model.to(device)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "for sentence in texts_test:\n",
    "    predictions.append(predict([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HId_35BKPUlw",
    "outputId": "73b674a0-dc4f-4384-da17-d7bf103b216f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       156\n",
      "           1       0.72      0.72      0.72       179\n",
      "           2       0.63      0.71      0.67       136\n",
      "           3       0.78      0.61      0.68       201\n",
      "           4       0.66      0.66      0.66       160\n",
      "           5       0.60      0.89      0.71        88\n",
      "\n",
      "    accuracy                           0.71       920\n",
      "   macro avg       0.71      0.73      0.71       920\n",
      "weighted avg       0.72      0.71      0.71       920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(predictions, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yF0X_utPJhT",
    "outputId": "51fbde81-ad7e-4fe3-a5ce-9ee3810ddfe7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[125,  25,   4,   1,   1,   0],\n",
       "       [ 17, 128,  23,   8,   1,   2],\n",
       "       [  0,  21,  96,  10,   4,   5],\n",
       "       [  0,   4,  24, 122,  38,  13],\n",
       "       [  0,   0,   5,  16, 106,  33],\n",
       "       [  0,   0,   0,   0,  10,  78]])"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(predictions, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Benp8RcRb7XV",
    "outputId": "7abd6ca7-c912-48dd-ae68-9ea38b601f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ainsi, Pierre a le privilège d'admirer chaque jour l'un des monuments les plus visités au monde !\n",
      "Je vais ensuite prendre ma douche dans ma salle-de-bain.\n",
      "À bientôt!\n",
      "Nous avons déménagé en France, parce qu'elle a toujours aimé la culture de ce pays.\n",
      "Je propose des spécialités de la région lyonnaise.\n",
      "J'imagine que les week-ends doivent être bien remplis !\n"
     ]
    }
   ],
   "source": [
    "# Loop for checking specific wrongs predictions \n",
    "for x, y, z in zip(predictions, labels_test, texts_test):\n",
    "    if int(x) == 0:\n",
    "        if y == 1:\n",
    "            print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sdo6ELiKTbZ",
    "outputId": "2f27a4dd-582d-4d2f-c660-d88138c10738"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu') \n",
    "model.to(device)\n",
    "\n",
    "# Predicts each word perceived difficulty in a sentence.\n",
    "predict(\"Dans un premier temps, nous nous demanderons si le travail n’est qu’une activité imposée par l’extérieur contre la volonté de l’Homme, puis dans un deuxième temps nous nous interrogerons sur le fait que le travail est une activité que l’être humain s’impose librement à lui-même.\".split(' '))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BSA2021_Google - LingoRank with CamemBERT v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02d2152201684bda9a3b1bde794166f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c2fd3d8311e43b28755b04f8e7835b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1e2470a159b4afca931d1966010628e",
      "placeholder": "​",
      "style": "IPY_MODEL_41288f2b2c10450194ee7e23f8fb28f3",
      "value": " 811k/811k [00:01&lt;00:00, 503kB/s]"
     }
    },
    "41288f2b2c10450194ee7e23f8fb28f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a0f60a85b6a487e89289827af5bc383": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6330973987784c2ab0770e421567aee8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6e45b357c334cae955efd3eace3bb4d",
       "IPY_MODEL_0c2fd3d8311e43b28755b04f8e7835b9"
      ],
      "layout": "IPY_MODEL_02d2152201684bda9a3b1bde794166f3"
     }
    },
    "c1e2470a159b4afca931d1966010628e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6e45b357c334cae955efd3eace3bb4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a0f60a85b6a487e89289827af5bc383",
      "max": 810912,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3e5447e64a54787a58ac4e0d214ad1a",
      "value": 810912
     }
    },
    "f3e5447e64a54787a58ac4e0d214ad1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
