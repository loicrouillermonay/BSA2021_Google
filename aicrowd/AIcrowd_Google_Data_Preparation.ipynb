{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNv3_EmgRGPH"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcYDp9SsQ7eY"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJRQGHFSREDj",
    "outputId": "340418e3-3932-4d2e-d9cf-8fafbdfbd634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "punctuation = string.punctuation\n",
    "stopwords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zX9DfaVRMew"
   },
   "outputs": [],
   "source": [
    "# import data \n",
    "train = pd.read_csv('/content/train.csv')\n",
    "test = pd.read_csv('/content/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QFndaNKRMcP"
   },
   "outputs": [],
   "source": [
    "# Entity Recognition\n",
    "def return_NER(sentence):\n",
    "    # Tokenize the sentence\n",
    "    doc = nlp(sentence)\n",
    "    # Return text and label for each sentence\n",
    "    return [(X.text, X.label_) for X in doc.ents]\n",
    "\n",
    "# Part-Of-Speech\n",
    "def return_POS(sentence):\n",
    "    # Tokenize the sentence\n",
    "    doc = nlp(sentence)\n",
    "    # Return tag of each token\n",
    "    return [(X, X.pos_) for X in doc]\n",
    "\n",
    "def NER_counter(sentence: string):\n",
    "    # Take a sentence with its text & label and couint each elements\n",
    "    ner = return_NER(sentence)\n",
    "    counter = Counter([t[1] for t in ner])\n",
    "    return counter\n",
    "\n",
    "def POS_counter(sentence: string):\n",
    "    # Take a token with its tags and count each elements\n",
    "    pos = return_POS(sentence)\n",
    "    counter = Counter([t[1] for t in pos])\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UcrSPOeRMZ1"
   },
   "outputs": [],
   "source": [
    "# Extraction of features to better understand the texts\n",
    "def features_extraction(dataframe: pd.DataFrame):\n",
    "    dataframe[\"num_chars\"] = dataframe[\"sentence\"].apply(len)\n",
    "    dataframe[\"num_words\"] = dataframe[\"sentence\"].apply(lambda x: len(x.split()))\n",
    "    dataframe[\"avg_word_length\"] = dataframe['sentence'].apply(lambda x: np.sum([len(w) for w in x.split()]) / len(x.split()))\n",
    "    dataframe['num_stopwords'] = dataframe['sentence'].apply(lambda x: np.sum([1 for word in x.split(' ') if word in stopwords]))\n",
    "    dataframe['ratio_num_words_over_stopwords'] = dataframe['num_words'] / dataframe['num_stopwords']\n",
    "    \n",
    "    # Iterate over each row in the dataframe and get some specific features\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # Part-Of-Speech\n",
    "        counter_pos = POS_counter(row['sentence'])\n",
    "        for x in counter_pos:\n",
    "            dataframe.loc[index, x] = counter_pos[x]\n",
    "        \n",
    "        # Entity Recognizer\n",
    "        counter_ner = NER_counter(row['sentence'])\n",
    "        for x in counter_ner:\n",
    "            dataframe.loc[index, x] = counter_ner[x]\n",
    "        \n",
    "        # Number of words before the first verb in each sentence\n",
    "        current_pos = return_POS(row['sentence'])\n",
    "        iter_current_pos = [str(y) for t in current_pos for y in t]\n",
    "        if 'VERB' in iter_current_pos:\n",
    "            dataframe.loc[index, 'num_words_before_first_verb'] = (iter_current_pos.index('VERB') + 1) // 2\n",
    "        else:\n",
    "            dataframe.loc[index, 'num_words_before_first_verb'] = 0\n",
    "            \n",
    "    return dataframe.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KCISV1QRMWR"
   },
   "outputs": [],
   "source": [
    "train_dataset = features_extraction(train)\n",
    "test_dataset = features_extraction(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2RoNgQdSlgS"
   },
   "source": [
    "# Cognates\n",
    "Cognate (of a word): having the same linguistic derivation as another (e.g. English father, German Vater, Latin pater ). (source: Oxford Languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PBxxyEIac5B"
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-translate==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAIu-H0tjL4n"
   },
   "outputs": [],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbAlw3G-jImF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance\n",
    "\n",
    "# Translate API\n",
    "from googleapiclient.discovery import build\n",
    "service = build('translate', 'v2', developerKey='YOUR-API-KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEIcZcJlaUSw"
   },
   "outputs": [],
   "source": [
    "for index, row in train_dataset.iterrows():\n",
    "    result = service.translations().list(source='fr', target='en', q=row.loc['sentence']).execute()\n",
    "    train_dataset.loc[index, 'Text_english_translation'] = result['translations'][0]['translatedText']\n",
    "\n",
    "for index, row in test_dataset.iterrows():\n",
    "    result = service.translations().list(source='fr', target='en', q=row.loc['sentence']).execute()\n",
    "    test_dataset.loc[index, 'Text_english_translation'] = result['translations'][0]['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4Itbk9RjhBG"
   },
   "outputs": [],
   "source": [
    "for index, row in train_dataset.iterrows():\n",
    "    train_dataset.loc[index, 'hamming'] = textdistance.hamming(row['sentence'], row['Text_english_translation'])\n",
    "    train_dataset.loc[index, 'hamming_normalized_similarity'] = textdistance.hamming.normalized_similarity(row['sentence'], row['Text_english_translation'])\n",
    "    train_dataset.loc[index, 'levenshtein'] = textdistance.levenshtein(row['sentence'], row['Text_english_translation'])\n",
    "    train_dataset.loc[index, 'levenshtein_normalized_similarity'] = textdistance.levenshtein.normalized_similarity(row['sentence'], row['Text_english_translation'])\n",
    "    train_dataset.loc[index, 'jaro_winkler'] = textdistance.jaro_winkler(row['sentence'], row['Text_english_translation'])\n",
    "\n",
    "    tokens_1 = row['sentence'].split()\n",
    "    tokens_2 = row['Text_english_translation'].split()\n",
    "    train_dataset.loc[index, 'jaccard'] = textdistance.jaccard(tokens_1 , tokens_2)\n",
    "    train_dataset.loc[index, 'sorensen'] = textdistance.sorensen(tokens_1 , tokens_2)\n",
    "\n",
    "for index, row in test_dataset.iterrows():\n",
    "    test_dataset.loc[index, 'hamming'] = textdistance.hamming(row['sentence'], row['Text_english_translation'])\n",
    "    test_dataset.loc[index, 'hamming_normalized_similarity'] = textdistance.hamming.normalized_similarity(row['sentence'], row['Text_english_translation'])\n",
    "    test_dataset.loc[index, 'levenshtein'] = textdistance.levenshtein(row['sentence'], row['Text_english_translation'])\n",
    "    test_dataset.loc[index, 'levenshtein_normalized_similarity'] = textdistance.levenshtein.normalized_similarity(row['sentence'], row['Text_english_translation'])\n",
    "    test_dataset.loc[index, 'jaro_winkler'] = textdistance.jaro_winkler(row['sentence'], row['Text_english_translation'])\n",
    "\n",
    "    tokens_1 = row['sentence'].split()\n",
    "    tokens_2 = row['Text_english_translation'].split()\n",
    "    test_dataset.loc[index, 'jaccard'] = textdistance.jaccard(tokens_1 , tokens_2)\n",
    "    test_dataset.loc[index, 'sorensen'] = textdistance.sorensen(tokens_1 , tokens_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "nRPvtD6qThjf",
    "outputId": "8a1bad4c-2e7a-4fbd-93f5-122ca6911480"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>ratio_num_words_over_stopwords</th>\n",
       "      <th>PRON</th>\n",
       "      <th>AUX</th>\n",
       "      <th>ADP</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>num_words_before_first_verb</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>PER</th>\n",
       "      <th>PART</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>MISC</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>Text_english_translation</th>\n",
       "      <th>hamming</th>\n",
       "      <th>hamming_normalized_similarity</th>\n",
       "      <th>levenshtein</th>\n",
       "      <th>levenshtein_normalized_similarity</th>\n",
       "      <th>jaro_winkler</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>sorensen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'est pour quand ?</td>\n",
       "      <td>A1</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It&amp;#39;s for when ?</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.620858</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Je pense que c'est bon.</td>\n",
       "      <td>A1</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I think this is good.</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.570324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'est pas mal.</td>\n",
       "      <td>A1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not bad.</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qu'est-ce que vous faites ?</td>\n",
       "      <td>A1</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>What are you doing ?</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.556790</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'est bien !</td>\n",
       "      <td>A1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It&amp;#39;s good !</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sentence difficulty  ...   jaccard  sorensen\n",
       "0           C'est pour quand ?         A1  ...  0.142857  0.250000\n",
       "1      Je pense que c'est bon.         A1  ...  0.000000  0.000000\n",
       "2               C'est pas mal.         A1  ...  0.000000  0.000000\n",
       "3  Qu'est-ce que vous faites ?         A1  ...  0.111111  0.200000\n",
       "4                 C'est bien !         A1  ...  0.200000  0.333333\n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "q2CET76aTjvk",
    "outputId": "396a3246-bbe8-407e-b473-625f5c449c90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>ratio_num_words_over_stopwords</th>\n",
       "      <th>PRON</th>\n",
       "      <th>AUX</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NUM</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>num_words_before_first_verb</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "      <th>X</th>\n",
       "      <th></th>\n",
       "      <th>PART</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>Text_english_translation</th>\n",
       "      <th>hamming</th>\n",
       "      <th>hamming_normalized_similarity</th>\n",
       "      <th>levenshtein</th>\n",
       "      <th>levenshtein_normalized_similarity</th>\n",
       "      <th>jaro_winkler</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>sorensen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Il est devenu courant de déplorer que la Franc...</td>\n",
       "      <td>380</td>\n",
       "      <td>65</td>\n",
       "      <td>4.861538</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.708333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It has become common to deplore the fact that ...</td>\n",
       "      <td>336.0</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.765480</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.096774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sur les flancs et les derrières de la noce, to...</td>\n",
       "      <td>97</td>\n",
       "      <td>18</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>On the flanks and behind the wedding party, th...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.647544</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J'aime aussi beaucoup sa voix, une voix un peu...</td>\n",
       "      <td>99</td>\n",
       "      <td>18</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I also really like her voice, a slightly deep ...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.633959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En partenariat avec l'INPES (Institut National...</td>\n",
       "      <td>224</td>\n",
       "      <td>35</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>In partnership with INPES (National Institute ...</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.120536</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.741265</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Il se réveilla seulement quand il entendit un ...</td>\n",
       "      <td>137</td>\n",
       "      <td>24</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>He only woke up when he heard a strange noise,...</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.255474</td>\n",
       "      <td>0.651604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  ...  sorensen\n",
       "0  Il est devenu courant de déplorer que la Franc...  ...  0.096774\n",
       "1  Sur les flancs et les derrières de la noce, to...  ...  0.060606\n",
       "2  J'aime aussi beaucoup sa voix, une voix un peu...  ...  0.000000\n",
       "3  En partenariat avec l'INPES (Institut National...  ...  0.030303\n",
       "4  Il se réveilla seulement quand il entendit un ...  ...  0.000000\n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JszzXcGJlNmB"
   },
   "outputs": [],
   "source": [
    "train_dataset.to_csv('train-augmented.csv', index = False)\n",
    "test_dataset.to_csv('test-augmented.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPqfATRH1MHI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AIcrowd_Google_Data_Preparation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
