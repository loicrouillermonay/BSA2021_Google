{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QDd0_NbhwSb",
    "outputId": "4dfa4fab-1f18-4dd5-a37d-935d63c02b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNS0ecn-cLBJ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.8.0\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "DKTsS8qmHKM0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "punctuation = string.punctuation\n",
    "# Credits to Olivier. (2021, January 5). Analyse de sentiments avec CamemBERT. Le Data Scientist. https://ledatascientist.com/analyse-de-sentiments-avec-camembert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvPWbsrRa9nJ",
    "outputId": "765e6352-8643-4e54-e1de-d3f4731706df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4800 entries, 0 to 4799\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   sentence    4800 non-null   object\n",
      " 1   difficulty  4800 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 75.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/content/train.csv\")\n",
    "test = pd.read_csv(\"/content/test.csv\")\n",
    "sample_submission = pd.read_csv(\"/content/sample_submission.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "shklKqs4cCyT"
   },
   "outputs": [],
   "source": [
    "# Transpore A1-C2 scale into 0 to 5 \n",
    "difficulties = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "for index, difficulty in zip(range(len(difficulties)), difficulties):\n",
    "    dataset['difficulty'] = dataset['difficulty'].replace([difficulty], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "oe53y0spnwdD"
   },
   "outputs": [],
   "source": [
    "# text preprocessing - remove punctuation and lowercase\n",
    "punctuation = punctuation.replace('-', '')\n",
    "punctuation = punctuation.replace('~', '')\n",
    "\n",
    "def preprocess_text(text: string):\n",
    "    sentence = ''.join([ word for word in text if word not in punctuation ])\n",
    "    return (sentence).lower()\n",
    "\n",
    "dataset.sentence = dataset.sentence.apply(lambda x: preprocess_text(x))\n",
    "test.sentence = test.sentence.apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "6gTRl3tFRXgA"
   },
   "outputs": [],
   "source": [
    "texts_train = dataset['sentence'].values.tolist()\n",
    "labels_train = dataset['difficulty'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "6a5vW39cfH51"
   },
   "outputs": [],
   "source": [
    "TOKENIZER = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Sfs9Ot9N4jdz"
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_texts, labels=None):\n",
    "    \"\"\"\n",
    "    Takes raw data as argument and returns a pytorch dataloader.\n",
    "\n",
    "    Args\n",
    "        raw_texts (array-like) : A list of texts in the form of 'str'\n",
    "        \n",
    "        labels : a labels list from 0 to 5\n",
    "    \n",
    "    Returns\n",
    "        inputs_ids, attention_masks, labels(optionel) : PyTorch object that contains tokenized and encoded versions of raw data\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_texts,\n",
    "                                                add_special_tokens=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if labels:\n",
    "        labels = torch.tensor(labels)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], labels\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "V_Sm1dvCddc6"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_mask, labels_train = preprocess(texts_train, labels_train)\n",
    "# Combine the training inputs into a TensorDataset\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "KaIdMxlirdFc"
   },
   "outputs": [],
   "source": [
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoTafgwY_pyB",
    "outputId": "3e3bd513-d991-4c38-8484-c6bb08ce621d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable to load trained model.\n",
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/aicrowd-v4.pt'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    state_dict = torch.load(\"/content/drive/MyDrive/Colab Notebooks/aicrowd-v4.pt\")\n",
    "    print(\"Loading trained model...\")\n",
    "    model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    state_dict=state_dict,\n",
    "    num_labels = 6)\n",
    "    print(\"Trained model loaded!\")\n",
    "except Exception as e:\n",
    "    print(\"Enable to load trained model.\")\n",
    "    print(e)\n",
    "    model = CamembertForSequenceClassification.from_pretrained(\n",
    "        'camembert-base',\n",
    "        num_labels = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "_7vUl8O2gAUM"
   },
   "outputs": [],
   "source": [
    "def predict(texts, model=model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(texts)\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "        return torch.argmax(retour[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "gjAyMcRW0bVW"
   },
   "outputs": [],
   "source": [
    "def evaluate(texts, labels, metric='report'):\n",
    "    predictions = predict(texts)\n",
    "    if metric == 'report':\n",
    "        return metrics.classification_report(labels, predictions, zero_division=0)\n",
    "    elif metric == 'matrix':\n",
    "        return metrics.confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "kEvYTvehrgkY"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "K0DoJqlPwbZS"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate - Default is 5e-5\n",
    "                  eps = 1e-8 # Adam Epsilon  - Default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "llBtKS_QbzXU"
   },
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELf3ehrNxY3k",
    "outputId": "7c018acb-a07c-4dad-81fc-2de4d9db9122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Epoch 0 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:23.\n",
      "  Batch 100  of  600    Elapsed: 0:00:46.\n",
      "  Batch 150  of  600    Elapsed: 0:01:10.\n",
      "  Batch 200  of  600    Elapsed: 0:01:34.\n",
      "  Batch 250  of  600    Elapsed: 0:01:58.\n",
      "  Batch 300  of  600    Elapsed: 0:02:22.\n",
      "  Batch 350  of  600    Elapsed: 0:02:47.\n",
      "  Batch 400  of  600    Elapsed: 0:03:11.\n",
      "  Batch 450  of  600    Elapsed: 0:03:36.\n",
      "  Batch 500  of  600    Elapsed: 0:04:01.\n",
      "  Batch 550  of  600    Elapsed: 0:04:25.\n",
      "\n",
      "  Average training loss: 1.35\n",
      "  Training epoch took: 290.33827900886536\n",
      "\n",
      "########## Epoch 1 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:20.\n",
      "  Batch 450  of  600    Elapsed: 0:03:46.\n",
      "  Batch 500  of  600    Elapsed: 0:04:11.\n",
      "  Batch 550  of  600    Elapsed: 0:04:36.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 1.00\n",
      "  Training epoch took: 302.55156230926514\n",
      "\n",
      "########## Epoch 2 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:21.\n",
      "  Batch 450  of  600    Elapsed: 0:03:46.\n",
      "  Batch 500  of  600    Elapsed: 0:04:11.\n",
      "  Batch 550  of  600    Elapsed: 0:04:36.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.80\n",
      "  Training epoch took: 302.4583420753479\n",
      "\n",
      "########## Epoch 3 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:31.\n",
      "  Batch 350  of  600    Elapsed: 0:02:56.\n",
      "  Batch 400  of  600    Elapsed: 0:03:21.\n",
      "  Batch 450  of  600    Elapsed: 0:03:46.\n",
      "  Batch 500  of  600    Elapsed: 0:04:11.\n",
      "  Batch 550  of  600    Elapsed: 0:04:36.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.60\n",
      "  Training epoch took: 302.7756519317627\n",
      "\n",
      "########## Epoch 4 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:31.\n",
      "  Batch 350  of  600    Elapsed: 0:02:56.\n",
      "  Batch 400  of  600    Elapsed: 0:03:21.\n",
      "  Batch 450  of  600    Elapsed: 0:03:46.\n",
      "  Batch 500  of  600    Elapsed: 0:04:11.\n",
      "  Batch 550  of  600    Elapsed: 0:04:36.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoch took: 302.84656977653503\n",
      "\n",
      "########## Epoch 5 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:31.\n",
      "  Batch 350  of  600    Elapsed: 0:02:56.\n",
      "  Batch 400  of  600    Elapsed: 0:03:21.\n",
      "  Batch 450  of  600    Elapsed: 0:03:46.\n",
      "  Batch 500  of  600    Elapsed: 0:04:11.\n",
      "  Batch 550  of  600    Elapsed: 0:04:36.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 302.9817154407501\n",
      "\n",
      "########## Epoch 6 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:21.\n",
      "  Batch 450  of  600    Elapsed: 0:03:46.\n",
      "  Batch 500  of  600    Elapsed: 0:04:11.\n",
      "  Batch 550  of  600    Elapsed: 0:04:36.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 302.47014713287354\n",
      "\n",
      "########## Epoch 7 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:20.\n",
      "  Batch 450  of  600    Elapsed: 0:03:45.\n",
      "  Batch 500  of  600    Elapsed: 0:04:10.\n",
      "  Batch 550  of  600    Elapsed: 0:04:35.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 302.3375811576843\n",
      "\n",
      "########## Epoch 8 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:20.\n",
      "  Batch 450  of  600    Elapsed: 0:03:45.\n",
      "  Batch 500  of  600    Elapsed: 0:04:10.\n",
      "  Batch 550  of  600    Elapsed: 0:04:35.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 301.95111632347107\n",
      "\n",
      "########## Epoch 9 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:20.\n",
      "  Batch 450  of  600    Elapsed: 0:03:45.\n",
      "  Batch 500  of  600    Elapsed: 0:04:10.\n",
      "  Batch 550  of  600    Elapsed: 0:04:35.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 301.71251249313354\n",
      "\n",
      "########## Epoch 10 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:20.\n",
      "  Batch 450  of  600    Elapsed: 0:03:45.\n",
      "  Batch 500  of  600    Elapsed: 0:04:10.\n",
      "  Batch 550  of  600    Elapsed: 0:04:35.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 301.43338775634766\n",
      "\n",
      "########## Epoch 11 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n",
      "  Batch 300  of  600    Elapsed: 0:02:30.\n",
      "  Batch 350  of  600    Elapsed: 0:02:55.\n",
      "  Batch 400  of  600    Elapsed: 0:03:20.\n",
      "  Batch 450  of  600    Elapsed: 0:03:45.\n",
      "  Batch 500  of  600    Elapsed: 0:04:10.\n",
      "  Batch 550  of  600    Elapsed: 0:04:35.\n",
      "Model saved!\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 301.23633074760437\n",
      "\n",
      "########## Epoch 12 / 40 ##########\n",
      "Training...\n",
      "  Batch 50  of  600    Elapsed: 0:00:25.\n",
      "  Batch 100  of  600    Elapsed: 0:00:50.\n",
      "  Batch 150  of  600    Elapsed: 0:01:15.\n",
      "  Batch 200  of  600    Elapsed: 0:01:40.\n",
      "  Batch 250  of  600    Elapsed: 0:02:05.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)  \n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "training_stats = []\n",
    "                                                                                \n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]\n",
    "# (Note that this is not the same as the number of training samples)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# This variable will evaluate the convergence on the training\n",
    "consecutive_epochs_with_no_improve = 0\n",
    "\n",
    "# Training\n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch} / {epochs} ##########')\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = time.time() - t0\n",
    "            \n",
    "            # Report progress\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}    Elapsed: {format_time(elapsed)}.')\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the 'device' using the 'to' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: skills \n",
    "        input_id = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear any previously calculated gradients before performing a backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch)\n",
    "        # the loss (because we provided skills) and the \"logits\"--the model\n",
    "        # outputs prior to activation\n",
    "        loss, logits = model(input_id, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. 'loss' is a Tensor containing a\n",
    "        # single value; the '.item()' function just returns the Python value \n",
    "        # from the tensor\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    "\n",
    "    if epoch > 0:\n",
    "        if min([stat['Training Loss'] for stat in training_stats]) <= avg_train_loss:\n",
    "            # i.e. If there is not improvement\n",
    "            consecutive_epochs_with_no_improve += 1\n",
    "        else:\n",
    "            # If there is improvement\n",
    "            consecutive_epochs_with_no_improve = 0\n",
    "            print(\"Model saved!\")\n",
    "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/aicrowd-v6.pt\")\n",
    "    \n",
    "    # Measure how long this epoch took\n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "    if consecutive_epochs_with_no_improve == 2:\n",
    "        print(\"Stop training : The loss has not changed since 2 epochs!\")\n",
    "        break\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Model saved!\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/metrics-aicrowd-v6.json', 'w+') as outfile:\n",
    "    json.dump(training_stats, outfile)\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/aicrowd-v6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhxOZwHtWeoS"
   },
   "outputs": [],
   "source": [
    "texts_test = test['sentence'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb0e_ENz8_Eq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu') \n",
    "model.to(device)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "for sentence in texts_test:\n",
    "    predictions.append(predict([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYfcaH-57rvY"
   },
   "outputs": [],
   "source": [
    "# Transpore 0-5 to A1-C2 scale\n",
    "for index, prediction in zip(range(len(predictions)), predictions):\n",
    "    if int(prediction) == 0:\n",
    "        sample_submission.loc[index, 'difficulty'] = 'A1'\n",
    "    if int(prediction) == 1:\n",
    "        sample_submission.loc[index, 'difficulty'] = 'A2'\n",
    "    if int(prediction) == 2:\n",
    "        sample_submission.loc[index, 'difficulty'] = 'B1'\n",
    "    if int(prediction) == 3:\n",
    "        sample_submission.loc[index, 'difficulty'] = 'B2'\n",
    "    if int(prediction) == 4:\n",
    "        sample_submission.loc[index, 'difficulty'] = 'C1'\n",
    "    if int(prediction) == 5:\n",
    "        sample_submission.loc[index, 'difficulty'] = 'C2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camembert-v4-e40-210505-1308.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import *\n",
    "today = datetime.now(tz=timezone.utc)\n",
    "date_time = today.strftime(\"%y%m%d-%H%M\")\n",
    "\n",
    "version = \"4\" #insert version here\n",
    "epoch = \"40\" #insert # epoch here\n",
    "submission_name = \"camembert-v%s-e%s-%s.csv\" % (version, epoch, date_time)\n",
    "print(submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjuPHblb7CCT"
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(submission_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ddd62KC9_3G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AIcrowd Google - CamemBERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
